{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "stemmer = PorterStemmer() #or SnowballStemmer(language='english')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../data/LEGAL_TEXT/\")\n",
    "print(len(files))\n",
    "master = pd.DataFrame(columns=['text','index','tid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    print(\" --------- #\",i+1,\": FILE \",files[i],\" --------\")\n",
    "    TID = (files[i])[:-4]\n",
    "    print(TID)\n",
    "    data = open(\"../data/LEGAL_TEXT/\"+files[i],'r')\n",
    "    d = data.read()\n",
    "    d = re.sub(r'\\n+','.',d)\n",
    "    d = re.sub(r' +',' ',d)\n",
    "    data = sent_tokenize(d)\n",
    "    df = pd.DataFrame(columns=['text','index','tid'])\n",
    "    for j in range(len(data)):\n",
    "        data[j] = re.sub(r'\\.+',' ',data[j])\n",
    "        data[j] = re.sub(r',+','',data[j])\n",
    "        if data[j]=='' or data[j]==' ':\n",
    "            data[j]='none'\n",
    "        df.loc[j,'text'] = data[j]\n",
    "        df.loc[j,'index'] = j\n",
    "        df.loc[j,'tid'] = TID\n",
    "    df = df[df.text!='none'].reset_index(drop=True)\n",
    "    for j in range(len(df)):\n",
    "        df.loc[j,'index']=str(j)\n",
    "    master = master.append(df)\n",
    "    #print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(master))\n",
    "for i in range(len(master)):\n",
    "    master.loc[i,'index'] = i\n",
    "master.to_csv(\"FOR_TF_IDF.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"FOR_TF_IDF.csv\")\n",
    "df=df.reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = str(df.loc[100000,'text'])\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=3, no_above=0.65, keep_n=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_1 = bow_corpus[1]\n",
    "for i in range(len(bow_doc_1)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], \n",
    "                                               dictionary[bow_doc_1[i][0]], \n",
    "bow_doc_1[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "counter=0\n",
    "for doc in corpus_tfidf:\n",
    "    print(\"===================== NUMBER: \",counter+1)\n",
    "    counter+=1\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv(\"FOR_TF_IDF.csv\")\n",
    "master=master.reset_index(drop=True)\n",
    "import re\n",
    "for i in range(len(master)):\n",
    "    print(\"------ LINE \",i+1,' OF ',len(master),\" ------\")\n",
    "    unseen_document = master.loc[i,'text']\n",
    "    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "    x=\"\"\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "        x = lda_model.print_topic(index, 10)\n",
    "        x = re.sub(r'\\*',' ',x)\n",
    "        x = re.sub(r'[0-9]','',x)\n",
    "        x = re.sub(r'\\+',' ',x)\n",
    "        x = re.sub(r'\\.',' ',x)\n",
    "        x = re.sub(r'\"',' ',x)\n",
    "        x = re.sub(r' +',' ',x)\n",
    "        x = re.sub(r' ',',',x)\n",
    "        break\n",
    "    master.loc[i,'tokens'] = x\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('graphrelated')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "83c8dc10edf69c1e63549dfe661d002820d3f59fa9b6f23d68f3e08d57562f6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
