{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzqKDQTDdV7N"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive/\", force_remount=True)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89kNQ5DPegBK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "#R = pd.DataFrame(columns=['PIPELINE/MODEL','TOKEN','LABEL'])\n",
        "\n",
        "def run_ner(TEXT_DATA):\n",
        "  #os.listdir(\"/content/drive/\")\n",
        "  #os.chdir(\"/content/drive/My Drive/IBM/\")\n",
        "  #files=os.listdir(os.getcwd())\n",
        "  #for i in range(len(files)):\n",
        "  #print(\"============= ------ \",files,\" ------ ==============\")\n",
        "  #filename = str(files)\n",
        "  #text=open(os.getcwd()+\"/\"+files,'r')\n",
        "  #TEXT_DATA = re.sub(' +', ' ',text.read())\n",
        "  result=[]\n",
        "  print(TEXT_DATA)\n",
        "  f = flairnlp(TEXT_DATA)\n",
        "  st = stanza_nlp(TEXT_DATA)\n",
        "  s1,s2,s3,s4,s5,s6 = sparknlp_models(TEXT_DATA)\n",
        "  #s5,s6 = sparknlp_models(TEXT_DATA)\n",
        "  result.append(f)\n",
        "  result.append(st)\n",
        "  result.append(s1)\n",
        "  result.append(s2)\n",
        "  result.append(s3)\n",
        "  result.append(s4)\n",
        "  result.append(s5)\n",
        "  result.append(s6)\n",
        "  return result\n",
        "    #print(\"Final data obtained: \",R)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec-4BUFmfF62"
      },
      "outputs": [],
      "source": [
        "!pip3 install flair\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "def flairnlp(text):\n",
        "  print(\"======================   FLAIR NLP   =========================\")\n",
        "  #print(\"Before:\",R)\n",
        "  print(\"step 1\")\n",
        "  #text=\"\"\n",
        "  sentence = Sentence(text)\n",
        "  print(\"step 2\")\n",
        "  tagger = SequenceTagger.load('ner')\n",
        "  print(\"step 3\")\n",
        "  tagger.predict(sentence)\n",
        "  #print(sentence)\n",
        "  #print(sentence.to_dict(tag_type='ner'))\n",
        "  entities = (sentence.to_dict(tag_type='ner'))[\"entities\"]\n",
        "  #print(entities)\n",
        "  t=[]\n",
        "  l=[]\n",
        "  print(len(entities))\n",
        "  for i in range(len(entities)):\n",
        "    t.append((entities[i])[\"text\"])\n",
        "    listToStr = ' '.join(map(str, (entities[i])[\"labels\"]))\n",
        "    listToStr = listToStr[0:3]\n",
        "    l.append(listToStr)\n",
        "  return update_labels(t,l,'FLAIR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ3ihrkXCRR7"
      },
      "outputs": [],
      "source": [
        "!pip3 install stanza\n",
        "import stanza\n",
        "\n",
        "def stanza_nlp(text):\n",
        "  print(\"======================   STANZA   =========================\")\n",
        "  stanza.download('en')\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
        "  doc = nlp(text)\n",
        "  t=[]\n",
        "  l=[]\n",
        "  #print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
        "  for sent in doc.sentences:\n",
        "    for ent in sent.ents:\n",
        "      t.append(ent.text)\n",
        "      l.append(ent.type)\n",
        "  #print(t)\n",
        "  #print(l)\n",
        "  return update_labels(t,l,'STANZA-ONTONOTES')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-tFIA8Kg3vH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp\n",
        "\n",
        "#! pip install spark-nlp --upgrade\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "import pandas as pd\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pTlfXeWg6tI"
      },
      "outputs": [],
      "source": [
        "def update_labels(tokens,labels,model_name):\n",
        "  R = pd.DataFrame(columns=['PIPELINE/MODEL','TOKEN','LABEL'])\n",
        "  print(\"=====================================================\")\n",
        "  print(\"=====================================================\")\n",
        "  print(\"=====================================================\")\n",
        "  print(\"              FOR THE MODEL \",model_name,\"           \")\n",
        "  print(\"\")\n",
        "  print(\" Number of tokens = \",len(tokens))\n",
        "  print(\" Number of labels = \",len(labels))\n",
        "  print(\"=====================================================\")\n",
        "  print(\"=====================================================\")\n",
        "  print(\"=====================================================\")\n",
        "\n",
        "\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i]=='O':\n",
        "      continue\n",
        "    else:\n",
        "      print(tokens[i],\" : \",labels[i])\n",
        "      X=len(R)\n",
        "      #print(\"Current Length: \",X)\n",
        "      R.loc[X,'PIPELINE/MODEL']=model_name\n",
        "      R.loc[X,'TOKEN']=tokens[i]\n",
        "      R.loc[X,'LABEL']=labels[i]\n",
        "      #print(\"New Lengrh: \",len(R))\n",
        "  #print(\"TABLE after model \",model_name,\": \")\n",
        "  #print(R)\n",
        "  return R\n",
        "\n",
        "def sparknlp_models(text):\n",
        "\n",
        "  spark=sparknlp.start()\n",
        "  sp=[]\n",
        "  print(\"======================   SPARK NLP   =========================\")\n",
        "  '''print(\"pipeline 1\")\n",
        "  pipeline1 = PretrainedPipeline('explain_document_dl', lang='en')\n",
        "  result = pipeline1.annotate(text)\n",
        "  t = list(result['token'])\n",
        "  ner = list(result['ner'])\n",
        "  S1 = update_labels(t,ner,'SPARK-explain_document_dl')'''\n",
        "  #sp.append(S1)\n",
        "  #for i in range(len(t)):\n",
        "  #  print(t[i],\" : \",ner[i])\n",
        "\n",
        "  '''pipeline4 = PretrainedPipeline('explain_document_ml', lang='en')\n",
        "  result = pipeline4.annotate(text)\n",
        "  t = list(result['token'])\n",
        "  l = list(result['ner'])\n",
        "  name = update_labels(t,l,'SPARK-explain_document_ml',name)'''\n",
        "\n",
        "  '''print(\"pipeline 2\")\n",
        "  pipeline2 = PretrainedPipeline('recognize_entities_dl', lang='en')\n",
        "  result = pipeline2.annotate(text)\n",
        "  t = list(result['token'])\n",
        "  l = list(result['ner'])\n",
        "  S2 = update_labels(t,l,'SPARK-recognize_entities_dl')'''\n",
        "  #sp.append(S2)\n",
        "  #for i in range(len(t)):\n",
        "  #  print(t[i],\" : \",ner[i])\n",
        "  \n",
        "  '''print(\"pipeline 3\")\n",
        "  pipeline3 = PretrainedPipeline('onto_recognize_entities_lg', lang='en')\n",
        "  result = pipeline3.annotate(text)\n",
        "  t = list(result['token'])\n",
        "  l = list(result['ner'])\n",
        "  S3 = update_labels(t,l,'SPARK-onto_recognize_entities_lg')'''\n",
        "  #sp.append(S3)\n",
        "  #for i in range(len(t)):\n",
        "  #  print(t[i],\" : \",ner[i])\n",
        "\n",
        "  #----------- self-made pipeline\n",
        "\n",
        "  print(\"pipeline 4\")\n",
        "  data = spark.createDataFrame([[text]]).toDF('text')\n",
        "  data.show(truncate=False)\n",
        "  document=DocumentAssembler().setInputCol('text').setOutputCol('document').setCleanupMode('shrink')\n",
        "  sentence=SentenceDetector().setInputCols('document').setOutputCol('sentence')\n",
        "  sentence.setExplodeSentences(True)\n",
        "  tokenizer=Tokenizer().setInputCols('sentence').setOutputCol('token')\n",
        "  checked=NorvigSweetingModel.pretrained().setInputCols(['token']).setOutputCol('checked')\n",
        "\n",
        "  #WordEmbeddingsModel.pretrained()\n",
        "  #embeddings=WordEmbeddingsModel.pretrained().setInputCols(['sentence','checked']).setOutputCol('embeddings')\n",
        "  embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "        .setInputCols(['document', 'token']) \\\n",
        "        .setOutputCol('embeddings')\n",
        "  #ner=NerDLModel.pretrained().setInputCols(['sentence','checked','embeddings']).setOutputCol('ner')\n",
        "  ner= NerDLModel.pretrained('ner_dl_bert', 'en') \\\n",
        "    .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "    .setOutputCol('ner')\n",
        "  converter=NerConverter().setInputCols(['sentence','checked','ner']).setOutputCol('chunk')\n",
        "  pipeline=Pipeline().setStages([document,sentence,tokenizer,checked,embeddings,ner,converter])\n",
        "  model=pipeline.fit(data)\n",
        "  result = model.transform(data)\n",
        "  #result.show()\n",
        "  tokens=result.select('token.result').toPandas()\n",
        "  #print(tokens['result'].values.tolist())\n",
        "  ftokens = [item for sublist in tokens['result'] for item in sublist]\n",
        "  #print(ftokens)\n",
        "  labels=result.select('ner.result').toPandas()\n",
        "  #print(labels['result'].values.tolist())\n",
        "  flabels = [item for sublist in labels['result'] for item in sublist]\n",
        "  #print(flabels)\n",
        "  S4 = update_labels(ftokens,flabels,'SPARK-self-ner_dl_bert')\n",
        "  #sp.append(S4)\n",
        "\n",
        "  print(\"pipeline 5\")\n",
        "  embeddings=WordEmbeddingsModel.pretrained().setInputCols(['sentence','checked']).setOutputCol('embeddings')\n",
        "  '''embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "        .setInputCols(['document', 'token']) \\\n",
        "        .setOutputCol('embeddings')'''\n",
        "  ner=NerDLModel.pretrained().setInputCols(['sentence','checked','embeddings']).setOutputCol('ner')\n",
        "  '''ner= NerDLModel.pretrained('ner_dl_bert', 'en') \\\n",
        "    .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "    .setOutputCol('ner')'''\n",
        "  converter=NerConverter().setInputCols(['sentence','checked','ner']).setOutputCol('chunk')\n",
        "  pipeline=Pipeline().setStages([document,sentence,tokenizer,checked,embeddings,ner,converter])\n",
        "  model=pipeline.fit(data)\n",
        "  result = model.transform(data)\n",
        "  #result.show()\n",
        "  tokens=result.select('token.result').toPandas()\n",
        "  #print(tokens['result'].values.tolist())\n",
        "  ftokens = [item for sublist in tokens['result'] for item in sublist]\n",
        "  #print(ftokens)\n",
        "  labels=result.select('ner.result').toPandas()\n",
        "  #print(labels['result'].values.tolist())\n",
        "  flabels = [item for sublist in labels['result'] for item in sublist]\n",
        "  #print(flabels)\n",
        "  S5 = update_labels(ftokens,flabels,'SPARK-self-ner_dl_GloVe')\n",
        "  #sp.append(S5)\n",
        "\n",
        "  print(\"pipeline 6\")\n",
        "    #WordEmbeddingsModel.pretrained()\n",
        "  #embeddings=WordEmbeddingsModel.pretrained().setInputCols(['sentence','checked']).setOutputCol('embeddings')\n",
        "  embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "        .setInputCols(['document', 'token']) \\\n",
        "        .setOutputCol('embeddings')\n",
        "  #ner=NerDLModel.pretrained().setInputCols(['sentence','checked','embeddings']).setOutputCol('ner')\n",
        "  ner= NerDLModel.pretrained('onto_bert_base_cased', 'en') \\\n",
        "    .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "    .setOutputCol('ner')\n",
        "  converter=NerConverter().setInputCols(['sentence','checked','ner']).setOutputCol('chunk')\n",
        "  pipeline=Pipeline().setStages([document,sentence,tokenizer,checked,embeddings,ner,converter])\n",
        "  model=pipeline.fit(data)\n",
        "  result = model.transform(data)\n",
        "  #result.show()\n",
        "  tokens=result.select('token.result').toPandas()\n",
        "  #print(tokens['result'].values.tolist())\n",
        "  ftokens = [item for sublist in tokens['result'] for item in sublist]\n",
        "  #print(ftokens)\n",
        "  labels=result.select('ner.result').toPandas()\n",
        "  #print(labels['result'].values.tolist())\n",
        "  flabels = [item for sublist in labels['result'] for item in sublist]\n",
        "  #print(flabels)\n",
        "  S6 = update_labels(ftokens,flabels,'SPARK-self-ner_dl_onto_bert_cased')\n",
        "  #sp.append(S6)\n",
        "\n",
        "  return S4,S5,S6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgsj6PUvjIpb"
      },
      "outputs": [],
      "source": [
        "#R = run_ner()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxrSOj4_GnbG"
      },
      "outputs": [],
      "source": [
        "def update_results(data):\n",
        "  #print(data)\n",
        "  new_data = pd.DataFrame(columns=['PIPELINE/MODEL','TOKEN','LABEL'])\n",
        "  size=0\n",
        "  for i in range(len(data)):\n",
        "    #print(\"----- i = \",i)\n",
        "    if (data.loc[i,'LABEL'])[0]=='B' and (data.loc[i,'LABEL'])[1]=='-':\n",
        "      j=i+1\n",
        "      #print(\"j = \",j)\n",
        "      new_label = (data.loc[i,'LABEL'])[2:]\n",
        "      #print(new_label)\n",
        "      entity=\"\"\n",
        "      while j<len(data)-1 and ((data.loc[j,'LABEL'])[0]=='I' and (data.loc[j,'LABEL'])[1]=='-'):\n",
        "        j=j+1\n",
        "        #print(\"j = \",j)\n",
        "      if j>i+1:\n",
        "        k=i\n",
        "        #print(\"initilazed k = \",k)\n",
        "        while(k<=j):\n",
        "          if k==j and (data.loc[k,'LABEL'])[0]=='B' and (data.loc[k,'LABEL'])[1]=='-':\n",
        "            break\n",
        "          entity=entity+\" \"+(data.loc[k,'TOKEN'])\n",
        "          k=k+1\n",
        "          #print(\"k = \",k)\n",
        "      new_data.loc[size,'PIPELINE/MODEL'] = data.loc[i,'PIPELINE/MODEL']\n",
        "      if entity!=\"\":\n",
        "        new_data.loc[size,'TOKEN'] = entity\n",
        "      else:\n",
        "        new_data.loc[size,'TOKEN'] = data.loc[i,'TOKEN']\n",
        "      new_data.loc[size,'LABEL'] = new_label\n",
        "      size=size+1\n",
        "      #print(\"size = \",size)\n",
        "  return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMqUoEuYkGiN"
      },
      "outputs": [],
      "source": [
        "def prepare_data(R):\n",
        "  X=[]\n",
        "  for i in range(len(R)):\n",
        "    print(R[i])\n",
        "    if i==0 or i==1:\n",
        "      X.append(R[i])\n",
        "      continue\n",
        "    X.append(update_results(R[i]))\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    print(\"##############\")\n",
        "    print(X[i])\n",
        "\n",
        "  new_data = pd.DataFrame(columns=['PIPELINE/MODEL','TOKEN','LABEL'])\n",
        "  for i in range(len(X)):\n",
        "    print(\"======================\")\n",
        "    print(X[i])\n",
        "    df=X[i]\n",
        "    for j in range(len(df)):\n",
        "      x = len(new_data)\n",
        "      new_data.loc[x,'PIPELINE/MODEL'] = df.loc[j,'PIPELINE/MODEL']\n",
        "      new_data.loc[x,'TOKEN'] = df.loc[j,'TOKEN']\n",
        "      new_data.loc[x,'LABEL'] = df.loc[j,'LABEL']\n",
        "  return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBIwJZ9gw51a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "#RESULT=[]\n",
        "TEMP=[]\n",
        "\n",
        "def main():\n",
        "  '''os.listdir(\"/content/drive/\")\n",
        "  os.chdir(\"/content/drive/My Drive/IBM/EXPERIMENT_1/TEXT_FILES/\")\n",
        "  files=os.listdir()\n",
        "  print(files)\n",
        "  result_matrices=[]\n",
        "  for i in range(len(files)):\n",
        "    if(files[i]!=\"ik_ipr.txt\"):\n",
        "      continue\n",
        "    print(files[i])\n",
        "    text=open(os.getcwd()+\"/\"+files[i],'r')\n",
        "    TEXT_DATA = re.sub(' +', ' ',text.read())\n",
        "    TEXT_DATA = re.sub(\"\\n\\n+\",\"\\n\",TEXT_DATA)'''\n",
        "    #entities=['judgement','article','section','clause','paragraph','court','defendant','prosecutor','plaintiff','involved entity','advocate','appellant','learned counsel','gazette','proceedings','judge','offence','accusation','objection','testify','jurisdiction','penalty','compensation','evidence']\n",
        "    #s=entities[0]\n",
        "    #for j in range(len(entities)):\n",
        "      #if j ==0:\n",
        "        #continue\n",
        "      #s=s+\" \"+entities[j]\n",
        "    #TEXT_DATA = s\n",
        "    #print(TEXT_DATA)\n",
        "    #TEXT_DATA = TEXT_DATA[0:100]\n",
        "    #print(TEXT_DATA)\n",
        "    #TEXT_DATA = TEXT_DATA.strip(\"\\n\")\n",
        "    #print(TEXT_DATA)\n",
        "    #non_empty_lines = [line for line in TEXT_DATA if line.strip() != \"\"]\n",
        "    #TEXT_DATA = non_empty_lines\n",
        "    #print(TEXT_DATA)'''\n",
        "  TEXT_DATA = 'However, on the basis of the material placed on record about the flagrant delay in conducting the investigation and prosecution of the accused in connection with the serious crime involving the death of a young married woman, this Court issued notice to the Director General of Police, Bihar as well as the Registrar General of the Patna High Court with a direction to them to place a report about the particulars of the present case, particularly with respect to the reasons behind such inordinate delay.'\n",
        "  R = run_ner(TEXT_DATA)\n",
        "  TEMP=R\n",
        "    #new_data = prepare_data(R)\n",
        "    #pd.set_option('display.max_rows', None)\n",
        "    #for j in range(len(new_data)):\n",
        "      #new_data.loc[j,'TOKEN'] = new_data.loc[j,'TOKEN'].strip()\n",
        "  R = prepare_data(R)\n",
        "  models = list(set(R['PIPELINE/MODEL']))\n",
        "  tokens = list(set(R['TOKEN']))\n",
        "  labels = list(set(R['LABEL']))\n",
        "  result = pd.DataFrame(index=tokens,columns = models)\n",
        "  for k in range(len(R)):\n",
        "    x = R.loc[k,'PIPELINE/MODEL']\n",
        "    y = R.loc[k,'TOKEN']\n",
        "    z = R.loc[k,'LABEL']\n",
        "    result.loc[y,x] = z\n",
        "  result = result.fillna(\"\")\n",
        "    #print(\"========== FILE: \",files[i],\" ===========\")\n",
        "  print(result)\n",
        "  result.to_csv(\"result.csv\",index=True)\n",
        "    #result_matrices.append(result)\n",
        "    #return result_matrices\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAZkQBgskVWQ"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "IBM-4:COMPARE ANNOTATORS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.15 ('graphrelated')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "83c8dc10edf69c1e63549dfe661d002820d3f59fa9b6f23d68f3e08d57562f6a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
